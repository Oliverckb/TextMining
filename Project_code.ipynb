{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkZeSck4MTed"
      },
      "source": [
        "# Text Mining Project\n",
        "test code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40YZFZPDdogb"
      },
      "source": [
        "**Load drive path**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rt3d6k4ewSWc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "path = \"/content/gdrive/My Drive/Text Mining/project\"  # your directory in google drive\n",
        "os.chdir(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8SiK21CdkYo"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import gensim\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "# frame = pd.read_json('News_Category_Dataset_v2.json',encoding='utf-8',orient='records',lines=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import**"
      ],
      "metadata": {
        "id": "z8X2Jug1_eBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "jkucmAGaDWxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import regularizers,layers,losses\n",
        "from collections import Counter\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import pydot\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras import Model\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Embedding, SpatialDropout1D, Conv1D, Flatten, Dense, Lambda, LSTM, concatenate,SimpleRNN,SpatialDropout1D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from numpy.ma.extras import average\n",
        "from tensorflow.python.keras.backend import mean\n",
        "import pickle\n",
        "from transformers import *\n",
        "from transformers import BertTokenizer, TFBertModel, BertConfig\n",
        "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "bert_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=2)"
      ],
      "metadata": {
        "id": "-_0wqJf6_dZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**new IMDb data**"
      ],
      "metadata": {
        "id": "zRd-FAPhBzj5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMDB_training_data = pd.read_csv('IMDB Dataset.csv')\n",
        "IMDB_training_data = pd.DataFrame(IMDB_training_data)\n",
        "\n",
        "IMDB_training_data"
      ],
      "metadata": {
        "id": "uyHHgfNBtKoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocessing**"
      ],
      "metadata": {
        "id": "e64d9fk4PY_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# remove punctuation\n",
        "def clean_text(text ): \n",
        "    delete_dict = {sp_character: '' for sp_character in string.punctuation} \n",
        "    delete_dict[' '] = ' ' \n",
        "    table = str.maketrans(delete_dict)\n",
        "    text1 = text.translate(table)\n",
        "    #print('cleaned:'+text1)\n",
        "    textArr= text1.split()\n",
        "    text2 = ' '.join([w for w in textArr if ( not w.isdigit() and  ( not w.isdigit() and len(w)>2))]) \n",
        "    \n",
        "    return text2.lower()\n",
        "\n",
        "## remove stop words and number    \n",
        "def preprocess(text):\n",
        "    # TODO: Replace the next line with your own code.\n",
        "    doc = nlp(text)\n",
        "    y = [token.lemma_ for token in doc if not token.is_stop and token.is_alpha]\n",
        "    # y = [token.lemma_ for token in doc if not token.is_stop and token.lemma_.isalpha()]\n",
        "    return y\n"
      ],
      "metadata": {
        "id": "-0YwatN-POWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# preprocessing and save the data\n",
        "tt = IMDB_training_data['review'].apply(clean_text).apply(preprocess)\n",
        "pickle.dump((tt),open('tokenized_IMDB.pkl','wb'))"
      ],
      "metadata": {
        "id": "H0JzODdVPS96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load the preprocessed data\n",
        "IMDB_token = pickle.load(open('tokenized_IMDB.pkl', 'rb'))\n",
        "for i in range(len(IMDB_token)):\n",
        "  IMDB_token[i] = ' '.join(IMDB_token[i])\n",
        "# print((IMDB_token))\n",
        "tt = IMDB_training_data\n",
        "tt['review'] = IMDB_token\n",
        "print(tt['review'][0])\n",
        "training_data, test_data, y_training, y_test = train_test_split(IMDB_token,\\\n",
        "                                                      IMDB_training_data['sentiment'],\\\n",
        "                                                      test_size=0.1,\\\n",
        "                                                      # stratify = train_Y.tolist(),\\\n",
        "                                                      shuffle=True,\\\n",
        "                                                      random_state=1)\n",
        "\n",
        "large_training_data, training_data, large_y_training, y_training = train_test_split(training_data,\\\n",
        "                                                      y_training,\\\n",
        "                                                      test_size=0.5,\\\n",
        "                                                      # stratify = train_Y.tolist(),\\\n",
        "                                                      shuffle=True,\\\n",
        "                                                      random_state=1)\n",
        "\n",
        "training_data, val_data, y_training, y_val = train_test_split(training_data,\\\n",
        "                                                      y_training,\\\n",
        "                                                      test_size=0.1,\\\n",
        "                                                      # stratify = train_Y.tolist(),\\\n",
        "                                                      shuffle=True,\\\n",
        "                                                      random_state=1)\n",
        "\n",
        "# len(training_data)"
      ],
      "metadata": {
        "id": "5e8TM58UPfnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to plot model training process\n",
        "def plot_modelfit_process(model_history, title):\n",
        "  n = len(model_history.history['accuracy'])\n",
        "  fig, ax = plt.subplots(2, 1, figsize=(8, 8))\n",
        "  ax[0].set_title('Accuracy')\n",
        "  ax[0].plot(model_history.history['accuracy'])\n",
        "  ax[0].plot(model_history.history['val_accuracy'])\n",
        "  ax[0].axvline(x=np.where(model_history.history['val_accuracy'] == np.max(model_history.history['val_accuracy']))[0][0],\n",
        "                color='k', linestyle='--')\n",
        "  ax[0].set_xlabel(\"Epochs\")\n",
        "  ax[0].set_xticks(range(n))\n",
        "  ax[0].legend(['train', 'validation'])\n",
        "\n",
        "  ax[1].set_title('Loss')\n",
        "  ax[1].plot(model_history.history['loss'])\n",
        "  ax[1].plot(model_history.history['val_loss'])\n",
        "  ax[1].axvline(x=np.where(model_history.history['val_loss'] == np.min(model_history.history['val_loss']))[0][0],\n",
        "                color='k', linestyle='--')\n",
        "  ax[1].set_xlabel(\"Epochs\")\n",
        "  ax[1].set_xticks(range(n))\n",
        "  ax[1].legend(['train', 'validation'])\n",
        "  fig.tight_layout(pad=2.0)\n",
        "  plt.show()\n",
        "  fig.savefig(path+title)"
      ],
      "metadata": {
        "id": "_FMgYXqW51so"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data description**"
      ],
      "metadata": {
        "id": "G9Ab_nRxCg6_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('-------Data--------')\n",
        "print(tt['sentiment'].value_counts())\n",
        "print(len(tt['sentiment']))\n",
        "print('-------------------')\n",
        "\n",
        "max_sentence_length  = max([len(str(text).split()) for text in tt['review']])\n",
        "\n",
        "print('Max Sentence Length :'+str(max_sentence_length))\n",
        "\n",
        "from IPython.core.pylabtools import figsize\n",
        "label,counts = np.unique(tt['sentiment'],return_counts=True)\n",
        "print(label)\n",
        "print(counts)\n",
        "\n",
        "ind_pos = np.where(tt['sentiment'] == 'positive')\n",
        "ind_neg = np.where(tt['sentiment'] == 'negative')\n",
        "# print(training_data.iloc[list(ind_pos)[0]])\n",
        "# print(len(list(ind_pos)))\n",
        "fig = plt.figure(1)\n",
        "plt.bar(label,counts)\n",
        "plt.xlabel(\"class label\", labelpad=18)\n",
        "plt.ylabel(\"number of documents\", labelpad=18)\n",
        "plt.title(\"Data Label Distribution\", y=1.02);\n",
        "fig.savefig(path+'/figures/total_label_distr.png')\n",
        "\n",
        "text_length  = [len(str(text).split()) for text in tt['review']]\n",
        "\n",
        "fig = plt.figure(2)\n",
        "plt.hist(x=text_length,bins=80,\n",
        "        color=\"steelblue\",\n",
        "        edgecolor=\"black\")\n",
        "plt.xlabel(\"text length\", labelpad=18)\n",
        "plt.ylabel(\"frequency\", labelpad=18)\n",
        "plt.title(\"Text Length Distribution\", y=1.02);\n",
        "fig.savefig(path+'/figures/total_text_len.png')\n",
        "\n",
        "positive_text_length  = [len(str(text).split()) for text in tt['review'].iloc[list(ind_pos)[0]]]  \n",
        "negative_text_length  = [len(str(text).split()) for text in tt['review'].iloc[list(ind_neg)[0]]]   \n",
        "\n",
        "fig,ax = plt.subplots(1,2, figsize=(12,6))\n",
        "fig.suptitle('Text Length')\n",
        "ax[0].hist(x=positive_text_length,bins=80,color=\"steelblue\",edgecolor=\"black\")\n",
        "ax[0].set(xlabel = 'Length', ylabel = 'frequency', title = 'Positive Reviews')\n",
        "ax[1].hist(x=negative_text_length,bins=80,color=\"steelblue\",edgecolor=\"black\")\n",
        "ax[1].set(xlabel = 'Length', ylabel = 'frequency', title = 'Negative Reviews')\n",
        "fig.savefig(path+'/figures/two_class_text_len.png')\n",
        "\n"
      ],
      "metadata": {
        "id": "q0WpWNivu9qw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('-------Train data--------')\n",
        "print(y_training.value_counts())\n",
        "print(len(y_training))\n",
        "print('-------------------------')\n",
        "\n",
        "# training_data = training_data.apply(clean_text)\n",
        "max_train_sentence_length  = max([len(str(text).split()) for text in training_data])\n",
        "# print(np.max(max_train_sentence_length))\n",
        "\n",
        "print('-------Test data--------')\n",
        "print(y_test.value_counts())\n",
        "print(len(y_test))\n",
        "print('-------------------------')\n",
        "\n",
        "# test_data = test_data.apply(clean_text)\n",
        "max_test_sentence_length  = max([len(str(text).split()) for text in test_data])\n",
        "\n",
        "print('Train Max Sentence Length :'+str(max_train_sentence_length))\n",
        "print('Test Max Sentence Length :'+str(max_test_sentence_length))"
      ],
      "metadata": {
        "id": "yk4w1XYySs_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.core.pylabtools import figsize\n",
        "label,counts = np.unique(y_training,return_counts=True)\n",
        "print(label)\n",
        "print(counts)\n",
        "\n",
        "ind_pos = np.where(y_training == 'positive')\n",
        "ind_neg = np.where(y_training == 'negative')\n",
        "fig = plt.figure(1)\n",
        "plt.bar(label,counts)\n",
        "plt.xlabel(\"class label\", labelpad=18)\n",
        "plt.ylabel(\"number of documents\", labelpad=18)\n",
        "plt.title(\"Data Label Distribution\", y=1.02);\n",
        "fig.savefig(path+'/figures/label_distr.png')\n",
        "\n",
        "train_text_length  = [len(str(text).split()) for text in training_data]\n",
        "\n",
        "fig = plt.figure(2)\n",
        "plt.hist(x=train_text_length,bins=80,\n",
        "        color=\"steelblue\",\n",
        "        edgecolor=\"black\")\n",
        "plt.xlabel(\"text length\", labelpad=18)\n",
        "plt.ylabel(\"frequency\", labelpad=18)\n",
        "plt.title(\"Text Length Distribution\", y=1.02);\n",
        "fig.savefig(path+'/figures/total_text_len.png')\n",
        "\n",
        "train_positive_text_length  = [len(str(text).split()) for text in training_data.iloc[list(ind_pos)[0]]]   \n",
        "train_negative_text_length  = [len(str(text).split()) for text in training_data.iloc[list(ind_neg)[0]]]   \n",
        "\n",
        "fig,ax = plt.subplots(1,2, figsize=(12,6))\n",
        "fig.suptitle('Text Length')\n",
        "ax[0].hist(x=train_positive_text_length,bins=80,color=\"steelblue\",edgecolor=\"black\")\n",
        "ax[0].set(xlabel = 'Length', ylabel = 'frequency', title = 'Positive Reviews')\n",
        "ax[1].hist(x=train_negative_text_length,bins=80,color=\"steelblue\",edgecolor=\"black\")\n",
        "ax[1].set(xlabel = 'Length', ylabel = 'frequency', title = 'Negative Reviews')\n",
        "fig.savefig(path+'/figures/text_len.png')"
      ],
      "metadata": {
        "id": "9VwQkEpVClDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive_text_freq = training_data.iloc[list(ind_pos)[0]].str.split(expand=True).unstack().value_counts(ascending=True)\n",
        "negative_text_freq = training_data.iloc[list(ind_neg)[0]].str.split(expand=True).unstack().value_counts(ascending=True)\n",
        "\n",
        "fig,ax = plt.subplots(1,2, figsize=(12,13))\n",
        "ax[0].barh(range(50),positive_text_freq[-50:], tick_label = positive_text_freq.keys()[-50:])\n",
        "ax[0].set(title = '')\n",
        "ax[1].barh(range(50),negative_text_freq[-50:], tick_label = negative_text_freq.keys()[-50:])\n",
        "fig.savefig(path+'/figures/word frequency.png')"
      ],
      "metadata": {
        "id": "ck5VoKG4SYvv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_words = 1500000\n",
        "\n",
        "tokenizer = Tokenizer(num_words=num_words,oov_token=\"unk\")\n",
        "tokenizer.fit_on_texts(training_data.tolist())\n",
        "vocab = tokenizer.word_index\n",
        "\n",
        "print(str(tokenizer.texts_to_sequences(['xyz how are you'])))"
      ],
      "metadata": {
        "id": "7yQMWcXIC9le"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gOvRd0V8L-t9"
      },
      "source": [
        "*import*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJk_CkyhL99B"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guWDg3xlBI7s"
      },
      "source": [
        "*Naive Bayes*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vm6oHlBg-nLA"
      },
      "outputs": [],
      "source": [
        "pipe = Pipeline([('vector',CountVectorizer()),('NB',MultinomialNB())])\n",
        "NB_model = pipe.fit(training_data,y_training)\n",
        "NB_y_pred = pipe.predict(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O914ZLPXEliN"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_test, NB_y_pred))\n",
        "\n",
        "cm = confusion_matrix(y_test, NB_y_pred)\n",
        "display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"negative\", \"positive\"])\n",
        "display.plot()\n",
        "plt.show()\n",
        "display.im_.figure.savefig(path+\"/NB cm.png\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = Pipeline([('vector',TfidfVectorizer()),('NB',MultinomialNB())])\n",
        "NB_model = pipe.fit(training_data,y_training)\n",
        "NB_y_pred = pipe.predict(test_data)\n",
        "\n",
        "print(classification_report(y_test, NB_y_pred))\n",
        "\n",
        "cm = confusion_matrix(y_test, NB_y_pred)\n",
        "display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"negative\", \"positive\"])\n",
        "display.plot()\n",
        "plt.show()\n",
        "display.im_.figure.savefig(path+\"/NB_tfidf cm.png\")"
      ],
      "metadata": {
        "id": "EoS8AjFoGxPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross Validation\n",
        "parameters = {'vector__binary':[0,1], 'vector__ngram_range':[(1,1), (1,2),(2,2)],'NB__alpha':[0.1,1]}\n",
        "\n",
        "Grid_S = GridSearchCV(pipe, param_grid = parameters,cv = 5)\n",
        "res = Grid_S.fit(val_data,y_val)"
      ],
      "metadata": {
        "id": "RbNQTd1ISac_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res.best_params_"
      ],
      "metadata": {
        "id": "emWeUXtMGAye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# optimal parameters\n",
        "\n",
        "pipe = Pipeline([('vector',TfidfVectorizer(binary = 1, ngram_range=(1,2))),('NB',MultinomialNB(alpha = 1))])\n",
        "NB_model = pipe.fit(training_data,y_training)\n",
        "NB_y_pred = pipe.predict(test_data)\n",
        "\n",
        "print(classification_report(y_test, NB_y_pred))\n",
        "\n",
        "cm = confusion_matrix(y_test, NB_y_pred)\n",
        "display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"negative\", \"positive\"])\n",
        "display.plot()\n",
        "plt.show()\n",
        "display.im_.figure.savefig(path+\"/NB_optimal cm.png\")\n"
      ],
      "metadata": {
        "id": "3R5_fvUqWBVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_wrong_NB = test_data[NB_y_pred!=y_test]\n",
        "print((df_wrong_NB.index))\n",
        "print((df_wrong_NB))"
      ],
      "metadata": {
        "id": "Qn068Q6C7vh2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1pWMHJPINVMp"
      },
      "source": [
        "*CNN*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7Z73EtqNUe3"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import string\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import losses\n",
        "from collections import Counter\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow.keras import preprocessing\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import pydot\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras import Model\n",
        "import sklearn.metrics\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Embedding, SpatialDropout1D, Conv1D, Flatten, Dense, Lambda, LSTM, concatenate,SimpleRNN,SpatialDropout1D\n",
        "import keras.backend as K\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYD5-WdirZdD"
      },
      "source": [
        "*plot dataset*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09JrgDvey-Ic"
      },
      "outputs": [],
      "source": [
        "#padding\n",
        "X_train = np.array( tokenizer.texts_to_sequences(training_data) )\n",
        "X_valid = np.array( tokenizer.texts_to_sequences(val_data) )\n",
        "X_test  = np.array( tokenizer.texts_to_sequences(test_data.tolist()) )\n",
        "\n",
        "X_train = pad_sequences(X_train, padding='post', maxlen=256)\n",
        "X_valid = pad_sequences(X_valid, padding='post', maxlen=256)\n",
        "X_test = pad_sequences(X_test, padding='post', maxlen=256)\n",
        "\n",
        "le = LabelEncoder()\n",
        "\n",
        "all_labels = le.fit_transform(y_training.to_list())\n",
        "all_labels_onehot = np.asarray( tf.keras.utils.to_categorical(all_labels))\n",
        "print(all_labels)\n",
        "train_labels = le.transform(y_training)\n",
        "train_labels_onehot = np.asarray( tf.keras.utils.to_categorical(train_labels))\n",
        "print(train_labels.shape)\n",
        "\n",
        "valid_labels = le.transform(y_val)\n",
        "valid_labels_onehot = np.asarray( tf.keras.utils.to_categorical(valid_labels))\n",
        "\n",
        "test_labels = le.transform(y_test.tolist())\n",
        "test_labels_onehot = np.asarray(tf.keras.utils.to_categorical(test_labels))\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((X_train,train_labels))\n",
        "valid_ds = tf.data.Dataset.from_tensor_slices((X_valid,valid_labels))\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((X_test,test_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VB6fpsCt0RnV"
      },
      "outputs": [],
      "source": [
        "train_labels_bi = le.fit_transform(y_training)\n",
        "train_labels = np.asarray( tf.keras.utils.to_categorical(train_labels_bi))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eQckHBqbQbC"
      },
      "source": [
        "pretrained embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOVbDBXWIO4_"
      },
      "outputs": [],
      "source": [
        "# read pretrained embedding\n",
        "embeddings_index = {}\n",
        "f = open(os.path.join( 'glove.twitter.27B.100d.txt'))\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "# build embedding matrix\n",
        "word_index = tokenizer.word_index\n",
        "EMBEDDING_DIM = 100\n",
        "\n",
        "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_Nuwd7oBdZQ"
      },
      "outputs": [],
      "source": [
        "def TextCNN_model_2(x_train_padded_seqs,y_train_onehot,x_valid_padded_seqs,y_valid_onehot,x_test_padded_seqs,y_test_onehot):\n",
        "\n",
        "    main_input = Input(shape=(256,), dtype='float64')\n",
        "    # 词嵌入（使用预训练的词向量）\n",
        "    embedder = layers.Embedding(len(vocab) + 1, 100, input_length=256, weights=[embedding_matrix], trainable=True)\n",
        "    embed = embedder(main_input)\n",
        "\n",
        "    # 词窗大小分别为3,4,5\n",
        "    cnn1 = layers.Conv1D(650, 3, padding='same', strides=1, activation='relu')(embed)\n",
        "    cnn1 = layers.MaxPooling1D(pool_size=198)(cnn1)\n",
        "    cnn1 = layers.Dropout(0.5)(cnn1)\n",
        "    cnn2 = layers.Conv1D(650, 4, padding='same', strides=1, activation='relu')(embed)\n",
        "    cnn2 = layers.MaxPooling1D(pool_size=196)(cnn2)\n",
        "    cnn2 = layers.Dropout(0.5)(cnn2)\n",
        "    cnn3 = layers.Conv1D(650, 5, padding='same', strides=1, activation='relu')(embed)\n",
        "    cnn3 = layers.MaxPooling1D(pool_size=194)(cnn3)\n",
        "    cnn3 = layers.Dropout(0.5)(cnn3)\n",
        "    cnn4 = layers.Conv1D(650, 6, padding='same', strides=1, activation='relu')(embed)\n",
        "    cnn4 = layers.MaxPooling1D(pool_size=192)(cnn4)\n",
        "    cnn4 = layers.Dropout(0.5)(cnn4)\n",
        "    # 合并三个模型的输出向量\n",
        "    cnn = layers.concatenate([cnn1, cnn2, cnn3,cnn4], axis=-1)\n",
        "    flat = layers.Flatten()(cnn)\n",
        "    drop = layers.Dropout(0.2)(flat)\n",
        "    main_output = layers.Dense(2, activation='softmax')(drop)\n",
        "    model = Model(inputs=main_input, outputs=main_output)\n",
        "    optim = Adam(lr=0.001)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optim, metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    \n",
        "    return model\n",
        "\n",
        "model = TextCNN_model_2(X_train,train_labels_onehot,X_valid,valid_labels_onehot,X_test,test_labels_onehot)\n",
        "\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\n",
        "mc = ModelCheckpoint(path+'/best_model_cnn1.h5', monitor='val_acc', mode='max',\n",
        "                     save_best_only=True, verbose=1)\n",
        "    \n",
        "valid_ds = tf.data.Dataset.from_tensor_slices((X_valid,valid_labels_onehot))\n",
        "\n",
        "history = model.fit(X_train,\n",
        "                    train_labels_onehot,\n",
        "                    batch_size=400,\n",
        "                    epochs=7,\n",
        "                    validation_data = valid_ds.batch(400),\n",
        "                    callbacks = [es,mc])\n",
        "\n",
        "result = model.predict(X_test)  # 预测样本属于每个类别的概率\n",
        "cnn1_result_labels = np.argmax(result, axis=1)  # 获得最大概率对应的标签\n",
        "print(classification_report(test_labels, cnn1_result_labels))\n",
        "# plot_modelfit_process(history, \"/training process CNN.png\")\n",
        "cm = confusion_matrix(test_labels, cnn1_result_labels)\n",
        "display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"negative\", \"positive\"])\n",
        "display.plot()\n",
        "plt.show()\n",
        "display.im_.figure.savefig(path+\"/CNN1 cm.png\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wrong_cnn = test_data[cnn1_result_labels!=test_labels]\n",
        "print((wrong_cnn.index[:100]))\n",
        "print((wrong_cnn))"
      ],
      "metadata": {
        "id": "eltsGX3Z-t8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwXa-CSRIEZ6"
      },
      "outputs": [],
      "source": [
        "wrong_cnn = test_data[cnn1_result_labels!=test_labels]\n",
        "print(wrong_cnn[18116])\n",
        "print(IMDB_training_data['sentiment'][18116])\n",
        "print(IMDB_training_data['review'][18116])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nwxyJDFngGnq"
      },
      "outputs": [],
      "source": [
        "#构建TextCNN模型\n",
        "#模型结构：词嵌入-卷积池化*3-拼接-全连接-dropout-全连接\n",
        "\n",
        "def TextCNN_model_1(x_train_padded_seqs,y_train_onehot,x_valid_padded_seqs,y_valid_onehot,x_test_padded_seqs,y_test_onehot):\n",
        "    main_input = Input(shape=(256,), dtype='float64')\n",
        "    # 词嵌入（使用预训练的词向量）\n",
        "    embedder = layers.Embedding(len(vocab) + 1, 100, input_length=256, trainable=True)\n",
        "    embed = embedder(main_input)\n",
        "    # 词窗大小分别为3,4,5\n",
        "    cnn1 = layers.Conv1D(650, 3, padding='same', strides=1, activation='relu')(embed)\n",
        "    cnn1 = layers.MaxPooling1D(pool_size=196)(cnn1)\n",
        "    cnn1 = layers.Dropout(0.5)(cnn1)\n",
        "    cnn2 = layers.Conv1D(650, 4, padding='same', strides=1, activation='relu')(embed)\n",
        "    cnn2 = layers.MaxPooling1D(pool_size=194)(cnn2)\n",
        "    cnn2 = layers.Dropout(0.5)(cnn2)\n",
        "    cnn3 = layers.Conv1D(650, 5, padding='same', strides=1, activation='relu')(embed)\n",
        "    cnn3 = layers.MaxPooling1D(pool_size=192)(cnn3)\n",
        "    cnn3 = layers.Dropout(0.5)(cnn3)\n",
        "    cnn4 = layers.Conv1D(650, 6, padding='same', strides=1, activation='relu')(embed)\n",
        "    cnn4 = layers.MaxPooling1D(pool_size=190)(cnn4)\n",
        "    cnn4 = layers.Dropout(0.5)(cnn4)\n",
        "    # 合并三个模型的输出向量\n",
        "    cnn = layers.concatenate([cnn1, cnn2, cnn3,cnn4], axis=-1)\n",
        "    flat = layers.Flatten()(cnn)\n",
        "    drop = layers.Dropout(0.2)(flat)\n",
        "    main_output = layers.Dense(2, activation='softmax')(drop)\n",
        "    optim = Adam(lr=0.0005)\n",
        "    model = Model(inputs=main_input, outputs=main_output)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optim, metrics=['accuracy'])\n",
        "    model.summary()\n",
        "\n",
        "    return model\n",
        "\n",
        "model = TextCNN_model_1(X_train,train_labels_onehot,X_valid,valid_labels_onehot,X_test,test_labels_onehot)\n",
        "valid_ds = tf.data.Dataset.from_tensor_slices((X_valid,valid_labels_onehot))\n",
        "\n",
        "history = model.fit(X_train, \n",
        "                    train_labels_onehot, \n",
        "                    batch_size=400, \n",
        "                    epochs=3,\n",
        "                    validation_data = valid_ds.batch(400),\n",
        "                    callbacks = [es,mc])\n",
        "result = model.predict(X_test)  # 预测样本属于每个类别的概率\n",
        "cnn2_result_labels = np.argmax(result, axis=1)  # 获得最大概率对应的标签\n",
        "\n",
        "print(classification_report(test_labels, cnn2_result_labels))\n",
        "# plot_modelfit_process(history, \"/training process CNN2.png\")\n",
        "cm = confusion_matrix(test_labels, cnn2_result_labels)\n",
        "display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"negative\", \"positive\"])\n",
        "display.plot()\n",
        "plt.show()\n",
        "display.im_.figure.savefig(path+\"/CNN2 cm.png\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wrong_cnn = test_data[cnn2_result_labels!=test_labels]\n",
        "print((wrong_cnn.index))\n",
        "print((wrong_cnn))"
      ],
      "metadata": {
        "id": "ed6nG3CKVGX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wrong_cnn = test_data[cnn2_result_labels!=test_labels]\n",
        "print(wrong_cnn[12561])\n",
        "# print(cnn1_result_labels[2])\n",
        "print(IMDB_training_data['sentiment'][12561])\n",
        "print(IMDB_training_data['review'][12561])"
      ],
      "metadata": {
        "id": "bO9BXfVKVHPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIuaVuwqZy0u"
      },
      "source": [
        "*RNN*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7fhsKOUWa1N"
      },
      "outputs": [],
      "source": [
        "def RNN_model(x_train_padded_seqs,y_train_onehot,x_valid_padded_seqs,y_valid_onehot,x_test_padded_seqs,y_test_onehot):\n",
        "    main_input = Input(shape=(256,), dtype='float64')\n",
        "    embedder = layers.Embedding(len(vocab) + 1, 100, input_length=256, trainable=True, weights=[embedding_matrix])\n",
        "    embed = embedder(main_input)\n",
        "\n",
        "    rnn1 = layers.Bidirectional(LSTM(128))(embed)\n",
        "    drop = layers.Dropout(0.5)(rnn1)\n",
        "    flat = layers.Flatten()(drop)\n",
        "    dense1 = layers.Dense(768, activation=\"relu\")(drop)\n",
        "    drop2 = layers.Dropout(0.5)(dense1)\n",
        "    dense2 = layers.Dense(2, activation=\"sigmoid\")(drop2)\n",
        "\n",
        "    optim = Adam(lr=0.0005)\n",
        "    model = Model(inputs=main_input, outputs=dense2)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optim, metrics=['accuracy'])\n",
        "    model.summary()\n",
        "\n",
        "    return model\n",
        "model = RNN_model(X_train,train_labels_onehot,X_valid,valid_labels_onehot,X_test,test_labels_onehot)\n",
        "valid_ds = tf.data.Dataset.from_tensor_slices((X_valid,valid_labels_onehot))\n",
        "\n",
        "history = model.fit(X_train, train_labels_onehot, batch_size=400, epochs=6,validation_data = valid_ds.batch(400))\n",
        "result = model.predict(X_test)  # 预测样本属于每个类别的概率\n",
        "lstm_result_labels = np.argmax(result, axis=1)  # 获得最大概率对应的标签\n",
        "\n",
        "print(classification_report(test_labels, lstm_result_labels))\n",
        "# plot_modelfit_process(history, \"/training process RNN.png\")\n",
        "cm = confusion_matrix(test_labels, lstm_result_labels)\n",
        "display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"negative\", \"positive\"])\n",
        "display.plot()\n",
        "plt.show()\n",
        "display.im_.figure.savefig(path+\"/RNN cm.png\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wrong_lstm1 = test_data[lstm_result_labels!=test_labels]\n",
        "print((wrong_lstm1.index))\n",
        "print((wrong_lstm1))"
      ],
      "metadata": {
        "id": "ejQluTKFsKIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wrong_lstm1 = test_data[lstm_result_labels!=test_labels]\n",
        "print(wrong_lstm1[12561])\n",
        "print(IMDB_training_data['sentiment'][12561])\n",
        "print(IMDB_training_data['review'][12561])"
      ],
      "metadata": {
        "id": "5wN-c-xqsFGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def RNN2_model(x_train_padded_seqs,y_train_onehot,x_valid_padded_seqs,y_valid_onehot,x_test_padded_seqs,y_test_onehot):\n",
        "    main_input = Input(shape=(256,), dtype='float64')\n",
        "    embedder = layers.Embedding(len(vocab) + 1, 100, input_length=256, trainable=True)\n",
        "    embed = embedder(main_input)\n",
        "\n",
        "    rnn1 = layers.Bidirectional(LSTM(128))(embed)\n",
        "    drop = layers.Dropout(0.5)(rnn1)\n",
        "    flat = layers.Flatten()(drop)\n",
        "    dense1 = layers.Dense(768, activation=\"relu\")(drop)\n",
        "    drop2 = layers.Dropout(0.5)(dense1)\n",
        "    dense2 = layers.Dense(2, activation=\"sigmoid\")(drop2)\n",
        "\n",
        "    optim = Adam(lr=0.0005)\n",
        "    model = Model(inputs=main_input, outputs=dense2)\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optim, metrics=['accuracy'])\n",
        "    model.summary()\n",
        "\n",
        "    return model\n",
        "model = RNN2_model(X_train,train_labels_onehot,X_valid,valid_labels_onehot,X_test,test_labels_onehot)\n",
        "valid_ds = tf.data.Dataset.from_tensor_slices((X_valid,valid_labels_onehot))\n",
        "\n",
        "history = model.fit(X_train, train_labels_onehot, batch_size=400, epochs=3,validation_data = valid_ds.batch(400))\n",
        "result = model.predict(X_test)  # 预测样本属于每个类别的概率\n",
        "lstm2_result_labels = np.argmax(result, axis=1)  # 获得最大概率对应的标签\n",
        "\n",
        "print(classification_report(test_labels, lstm2_result_labels))\n",
        "# plot_modelfit_process(history, \"/training process RNN2.png\")\n",
        "cm = confusion_matrix(test_labels, lstm2_result_labels)\n",
        "display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"negative\", \"positive\"])\n",
        "display.plot()\n",
        "plt.show()\n",
        "display.im_.figure.savefig(path+\"/RNN2 cm.png\")"
      ],
      "metadata": {
        "id": "d9HpRJ8Y6k3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wrong_lstm2 = test_data[lstm2_result_labels!=test_labels]\n",
        "print((wrong_lstm2.index))\n",
        "print((wrong_lstm2))"
      ],
      "metadata": {
        "id": "W1H0x3lAr19F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wrong_lstm2 = test_data[lstm2_result_labels!=test_labels]\n",
        "print(wrong_lstm2[12561])\n",
        "# print(lstm2_result_labels[2])\n",
        "print(IMDB_training_data['sentiment'][12561])\n",
        "print(IMDB_training_data['review'][12561])"
      ],
      "metadata": {
        "id": "qbRDX9dKr-CZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bert**"
      ],
      "metadata": {
        "id": "JDWv4KgyaCgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Encoder_BERT(text_data,labels):\n",
        "  input_ids=[]\n",
        "  attention_masks=[]\n",
        "  sentences = text_data\n",
        "  labels=labels\n",
        "  len(sentences),len(labels)\n",
        "\n",
        "  for sent in sentences:\n",
        "      bert_inp=bert_tokenizer.encode_plus(sent,\n",
        "                                          add_special_tokens = True,\n",
        "                                          max_length =256,\n",
        "                                          pad_to_max_length = True,\n",
        "                                          return_attention_mask = True)\n",
        "      input_ids.append(bert_inp['input_ids'])\n",
        "      attention_masks.append(bert_inp['attention_mask'])\n",
        "\n",
        "  input_ids=np.asarray(input_ids)\n",
        "  attention_masks=np.array(attention_masks)\n",
        "  labels=np.array(labels)\n",
        "  return input_ids,attention_masks,labels"
      ],
      "metadata": {
        "id": "ExVJH3xQ3HRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tr_input_ids,tr_attention_masks,tr_labels = Encoder_BERT(training_data,y_training)\n",
        "va_input_ids,va_attention_masks,va_labels = Encoder_BERT(val_data,y_val)\n",
        "te_input_ids,te_attention_masks,te_labels = Encoder_BERT(test_data,y_test)"
      ],
      "metadata": {
        "id": "-4M9K-_swE-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a=le.fit_transform(tr_labels)"
      ],
      "metadata": {
        "id": "sfZv9WtvxdoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "print('Preparing the pickle file.....')\n",
        "\n",
        "# train validation test\n",
        "tr_pickle_inp_path='tr_bert_inp.pkl'\n",
        "tr_pickle_mask_path='tr_bert_mask.pkl'\n",
        "tr_pickle_label_path='tr_bert_label.pkl'\n",
        "\n",
        "va_pickle_inp_path='va_bert_inp.pkl'\n",
        "va_pickle_mask_path='va_bert_mask.pkl'\n",
        "va_pickle_label_path='va_bert_label.pkl'\n",
        "\n",
        "te_pickle_inp_path='te_bert_inp.pkl'\n",
        "te_pickle_mask_path='te_bert_mask.pkl'\n",
        "te_pickle_label_path='te_bert_label.pkl'\n",
        "\n",
        "pickle.dump((tr_input_ids),open(tr_pickle_inp_path,'wb'))\n",
        "pickle.dump((tr_attention_masks),open(tr_pickle_mask_path,'wb'))\n",
        "pickle.dump((tr_labels),open(tr_pickle_label_path,'wb'))\n",
        "\n",
        "pickle.dump((va_input_ids),open(va_pickle_inp_path,'wb'))\n",
        "pickle.dump((va_attention_masks),open(va_pickle_mask_path,'wb'))\n",
        "pickle.dump((va_labels),open(va_pickle_label_path,'wb'))\n",
        "\n",
        "pickle.dump((te_input_ids),open(te_pickle_inp_path,'wb'))\n",
        "pickle.dump((te_attention_masks),open(te_pickle_mask_path,'wb'))\n",
        "pickle.dump((te_labels),open(te_pickle_label_path,'wb'))\n",
        "\n",
        "# print('train Pickle files saved as ',pickle_inp_path,pickle_mask_path,pickle_label_path)"
      ],
      "metadata": {
        "id": "LGvGyPKjxQYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Pickles\n",
        "import pickle\n",
        "\n",
        "tr_pickle_inp_path='tr_bert_inp.pkl'\n",
        "tr_pickle_mask_path='tr_bert_mask.pkl'\n",
        "tr_pickle_label_path='tr_bert_label.pkl'\n",
        "\n",
        "va_pickle_inp_path='va_bert_inp.pkl'\n",
        "va_pickle_mask_path='va_bert_mask.pkl'\n",
        "va_pickle_label_path='va_bert_label.pkl'\n",
        "\n",
        "te_pickle_inp_path='te_bert_inp.pkl'\n",
        "te_pickle_mask_path='te_bert_mask.pkl'\n",
        "te_pickle_label_path='te_bert_label.pkl'\n",
        "\n",
        "print('Loading the saved pickle files..')\n",
        "\n",
        "tr_input_ids=pickle.load(open(tr_pickle_inp_path, 'rb'))\n",
        "tr_attention_masks=pickle.load(open(tr_pickle_mask_path, 'rb'))\n",
        "tr_labels=pickle.load(open(tr_pickle_label_path, 'rb'))\n",
        "\n",
        "va_input_ids=pickle.load(open(va_pickle_inp_path, 'rb'))\n",
        "va_attention_masks=pickle.load(open(va_pickle_mask_path, 'rb'))\n",
        "va_labels=pickle.load(open(va_pickle_label_path, 'rb'))\n",
        "\n",
        "te_input_ids=pickle.load(open(te_pickle_inp_path, 'rb'))\n",
        "te_attention_masks=pickle.load(open(te_pickle_mask_path, 'rb'))\n",
        "te_labels=pickle.load(open(te_pickle_label_path, 'rb'))\n",
        "\n",
        "print('Training Input shape {} Training Attention mask shape {} Training Input label shape {}'.format(tr_input_ids.shape,tr_attention_masks.shape,tr_labels.shape))\n",
        "print('Validation Input shape {} Validation  Attention mask shape {} Validation  Input label shape {}'.format(va_input_ids.shape,va_attention_masks.shape,va_labels.shape))\n",
        "print('Test Input shape {} Test Attention mask shape {} Test Input label shape {}'.format(te_input_ids.shape,te_attention_masks.shape,te_labels.shape))\n",
        "print(te_labels)"
      ],
      "metadata": {
        "id": "e-JvAEUnyr3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "pickle_inp_path='bert_inp.pkl'\n",
        "pickle_mask_path='bert_mask.pkl'\n",
        "pickle_label_path='bert_label.pkl'\n",
        "\n",
        "print('Loading the saved pickle files..')\n",
        "\n",
        "input_ids=pickle.load(open(pickle_inp_path, 'rb'))\n",
        "attention_masks=pickle.load(open(pickle_mask_path, 'rb'))\n",
        "labels=pickle.load(open(pickle_label_path, 'rb'))\n",
        "\n",
        "print('Input shape {} Attention mask shape {} Input label shape {}'.format(input_ids.shape,attention_masks.shape,labels.shape))\n",
        "print(labels)"
      ],
      "metadata": {
        "id": "ESiQ45DyG0d5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_dir='tb_bert'\n",
        "model_save_path='bert_model.h5'\n",
        "import keras\n",
        "# import tensorflow.keras.utils.plot_model\n",
        "\n",
        "callbacks = [tf.keras.callbacks.ModelCheckpoint(filepath=model_save_path,save_weights_only=True,monitor='val_loss',mode='min',save_best_only=True),keras.callbacks.TensorBoard(log_dir=log_dir)]\n",
        "\n",
        "print('\\nBert Model',bert_model.summary())\n",
        "\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=2e-5,epsilon=1e-08)\n",
        "\n",
        "bert_model.compile(loss=loss,optimizer=optimizer,metrics=[metric])\n",
        "\n",
        "tf.keras.utils.plot_model(bert_model, \n",
        "                          # to_file='/content/drive/MyDrive/Colab Notebooks/TextMining/bertmodel_plot.png', \n",
        "           show_shapes=True, \n",
        "           show_layer_names=False)"
      ],
      "metadata": {
        "id": "kKA66wpZHjhn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history=bert_model.fit([tr_input_ids,tr_attention_masks],le.fit_transform(tr_labels),batch_size=16,epochs=10,validation_data=([va_input_ids,va_attention_masks],le.fit_transform(va_labels)),callbacks=callbacks)\n",
        "plot_modelfit_process(history, \"/training process BERT.png\")"
      ],
      "metadata": {
        "id": "1GwvxhoRIqJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_save_path='bert_model.h5'\n",
        "\n",
        "trained_model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=2)\n",
        "trained_model.compile(loss=loss,optimizer=optimizer, metrics=[metric])\n",
        "trained_model.load_weights(model_save_path)\n",
        "\n",
        "preds = trained_model.predict([te_input_ids,te_attention_masks],batch_size=32)\n",
        "pred_labels = np.argmax(preds[0],axis=1)\n",
        "# f1 = f1_score(val_label, pred_labels)\n",
        "# print('F1 score',f1)\n",
        "print('Classification Report')\n",
        "print(classification_report(le.fit_transform(te_labels),pred_labels))\n",
        "print('Training and saving built model.....')  \n",
        "\n",
        "cm = confusion_matrix(test_labels, pred_labels)\n",
        "display = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"negative\", \"positive\"])\n",
        "display.plot()\n",
        "plt.show()\n",
        "display.im_.figure.savefig(path+\"/BERT cm.png\")"
      ],
      "metadata": {
        "id": "6WR5idmqq13-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wrong_bert = test_data[pred_labels!=test_labels]\n",
        "print((wrong_bert.index))\n",
        "print((wrong_bert))"
      ],
      "metadata": {
        "id": "NbnBMHyetRRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wrong_bert = test_data[pred_labels!=test_labels]\n",
        "print(wrong_bert[12561])\n",
        "# print(lstm2_result_labels[2])\n",
        "print(IMDB_training_data['sentiment'][12561])\n",
        "print(IMDB_training_data['review'][12561])"
      ],
      "metadata": {
        "id": "C2Dfl89Uux_U"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "Project test code.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}